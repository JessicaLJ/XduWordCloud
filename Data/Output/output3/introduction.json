[" It amounts to an incrementalmethod for dynamicprogrammingwhich imposes limited computational demands.", "It worksby successivelyimprovingits evaluationsof the qualityof particular actionsat particular states.", "Keywords.", "It can also be viewed as a method of asynchronous dynamic programming (DP).", "It provides agents withthe capability of learning to act optimally in Markovian domains by experiencing the con- sequences of actions, without requiring them to build maps of the domains.", "Learning proceeds similarly to Sutton's (1984; 1988) method of temporal differences (TD): an agent tries an action at a particular state, and evaluates its consequences in terms of the immediate reward or penalty it receives and its estimate of the value of the state to which it is taken.", "By trying all actions in all states repeatedly, it learns which are best overall, judged by long-term discounted reward.", "Sec- tion 2 describes the problem, the method, and the notation, section 3 gives an overviewof the proof, and section 4 discusses two extensions.", "Formal details are left as far as pos- sible to the appendix.", "See also Werbos (1977).", "55 280 c. WATKINS AND E DAYAN"]