[" The world constitutes a controlled Markov process with the agent as a controller.", "The task facing the agent is that of determining an optimal policy, one that maximizes total discounted expected reward.", "There are traditional methods (e.g., Sato, Abe & Takeda, 1988) for learning (Rx(a) and Pxy[a] while concurrently performing DP, but any assumption of certainty equivalence, i.e., calculating actions as if the current model were accurate, costs dearly in the early stages of learning (Barto & Singh, 1990).", "The most important condition implicit in the convergence theorem given below is that the sequence of episodes that forms the basis of learning must include an infinite number of episodes for each starting state and action.", "This may be considered a strong condition on the way states and actions are selected--however, under the stochastic conditions ofthe theorem, no method could be guaranteed to find an optimal policy under weaker con- ditions.", "Note, however, that the episodes need not form a continuous sequence--that is the y of one episode need not be the x of the next episode.", "Define ni(x, a) as the index of the ith time that action a is tried in state x.", "3.", "A formal description of the AFIP is given in the appendix, but the easiest way to think of it is in terms of a card game.", "All the cards together form an infinite deck, with the first episode-card next-to-bottom and stretching infinitely upwards, in order.", "The bottom card (numbered O) has written on it the agent's initial values Q0(x, a) for all pairs o f x and a.", "The actions permitted in the AFIP are the same as those permitted in the real process.", "First, all the cards for episodes later than n are eliminated, leaving just a finite deck.", "If the bottom card is reached, the game stops in a special, absorbing, state, and just provides the reward written on this card for x, a, namely Q0(x, a).", "Replaying the episode on card t consists of emitting the reward, rt, written on the card, and then moving to the next state (Yt, t - 1) in the AFIP, where Yt is the state to which the real process went on that episode.", "Card t itself is thrown away.", "The next state transition of the AFIP will be taken based on just the remaining deck.", "The above completely specifies how state transitions and rewards are determined in the AFIP.", "58 Q-LEARNING  283As defined above, the AFtP is as much a controlled Markov process as is the real pro-cess.", "Therefore, after a finite number of actions, the bottom card will always be reached.", "3.1.", "Lemmas Two lemmas form the heart of the proof.", "Informal statements of the lemmas and outlines of their proofs are given below; consult the appendix for the formal statements.", "The AFIP was directly constructed to have this property.", "The proof proceeds by backwards induction, following the AFIP down through the stack of past episodes.", "Lemma B Lemma B concerns the convergence of the AFIP to the real process.", "The first two steps are preparatory; the next two specify the form of the convergence and provide foundations for proving that it occurs.", "B.1 Consider a discounted, bounded-reward, finite Markov process.", "The AFIP effectively estimates the mean rewards and transitions of the real process over all the episodes.", "BAConsider executing a series of s actions in the AFIP and in the real process.", "The discrepancy in the action values over a finite number s of actions between the values of two approximately equal Markov processes grows at most quadratically with s. So, if the transition probabilities and rewards are close, then the values of the actions must be close too.", "3.2.", "1. .", ".", ", as at state x in the ARP, with Q(x, at .", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", ".", "The second term is the cost, from B.4, of the incorrect rewards and transition probabilities.", "Also since equation 4 applies to any set of ac-tions, it applies perforce to a set of actions optimal for either the AFIP or the real proc- ess.", "4.", "Discussions and conclusionsFor the sake of clarity, the theorem proved above was somewhat restricted.", "Two par-ticular extensions to the version of Q-learning described above have been used in prac- tice.", "The convergence result holds for both of these, and this section sketches the modifications to the proof that are necessary.", "A process with absorbing goal states has one or more states which are bound in the end to trap the agent.", "Since the process would always get trapped were it allowed to run, for every state x there is some number of actions u(x) such that no matter what they are, there is a probability p (x) > 0 of having reached one of the goal states after executing those actions.", ".", ".", "Changing more than one Q value on each iteration requires a minor modification to the action replay process AI:lP such that an action can be taken at any level at which it was executed in the real process--i.e., more than one action can be taken at each level.", "As long as the stochastic convergence conditions in equation 3 are still satisfied, the proof requires no non-trivial modification.", "The Qn(x, a) values are still optimal for the modified AFIP, and this still tends to the real process in the original manner.", "Intuitively, the proof relies on the AFIP estimating rewards and transition functions based on many episodes, and this is just speeded up by changing more than one Q value per iteration.", "Although the paper has so far presented an apparent dichotomy between Q-learning and methods based on certainty equivalence, such as Sato, Abe and Takeda (1988), in fact there is more of a continuum.", "If the agent can remember the details of its learning episodes, then, after altering the learning rates, it can use each of them more than once (which is equivalent to putting cards that were thrown away, back in, lower down on the AFIP stack).This biases the Q-learning process towards the particular sample of the rewards and transi- tions that it has experienced.", "In the limit of re-presenting 'old' cards infinitely often, this reuse amounts to the certainty equivalence step of calculating the optimal actions for the observed sample of the Markovian environment rather than the actual environment itself.", "The theorem above only proves the convergence of a restricted version of Watkins' (1989) comprehensive Q-learning algorithm, since it does not permit updates based on the rewards from more than one iteration.", "This addition was pioneered by Sutton (1984; 1988) in his TD(X) algorithm, in which a reward from a step taken r iterations previously is weighted by Xr, where X < 1.", "Unfortunately, the theorem does not extend trivially to this case, and alternative proof methods such as those in Kushner and Clark (1978) may be required.", "This paper has presented the proof outlined by Watkins (1989) that Q-learning converges with probability one under reasonable conditions on the learning rates and the Markovian environment.", "Such a guarantee has previously eluded most methods of reinforcement learning.", "Acknowledgments We are very grateful to Andy Barto, Graeme Mitchison, Steve Nowlan, Satinder Singh, Rich Sutton and three anonymous reviewers for their valuable comments on multifarious aspects of Q-learning and this paper.", "Support was from Philips Research Laboratories and SERC.", "PD's current address is CNL, The Salk Institute, PO Box 85800, San Diego, CA 92186-5800, USA.", "Notes 1.", "In general, the set of available actions may differ from state to state.", "Here we assume it does not, to simplify the notation.", "The theorem we present can straightfowardly be extended to the general case.", "2.", "The discount factor for the ARIa will be taken to be 3', the same as for the real process.", "3.", "Appendix The action-replay process The definition of the AFlP is contingent on a particular sequence of episodes observed in the real process.", "The stochastic reward and state transition consequent on performing action a at state Ix, n) is given as follows.", "Define if x, a has been executed before episode n : otherwise such that n i* is the last time before episode n that x, a was exeucted in the real process.", "If i.", "Otherwise, let r i. with probablity %i.", "i.", "This last point is crucial, taking an action in the AFlP always causes a state transition to a lower level--so it ultimately terminates.", "The discount factor in the AFIP is % the same as in the real process.", "Proof By induction.", "From the construction of the ARP, Q0(x, a) is the optimal--indeed the only possible--action value of (x, 0), a. a Now consider the cases in trying to perform action a in (x, n).", "This argument iterates for the second action with n 1 as the new lower limit.", "65 290 C. WATKINS AND P. DAYAN B. .", ".", ".", ".", "67 292 C. WATKINS AND P. DAYAN"]